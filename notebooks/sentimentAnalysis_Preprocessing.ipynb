{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Entrega final, ciencia de datos III </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manue\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manue\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: Textblob in c:\\users\\manue\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\manue\\anaconda3\\lib\\site-packages (from Textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk>=3.8->Textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk>=3.8->Textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk>=3.8->Textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manue\\anaconda3\\lib\\site-packages (from nltk>=3.8->Textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manue\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.8->Textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se iomportan las bibliotecas. \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se carga el dataset\n",
    "file_path=r'C:\\Users\\manue\\OneDrive\\Escritorio\\SentimentAnalysis\\data\\new_test_data_s140.csv'\n",
    "df=pd.read_csv(file_path, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales\n",
      "   Polarity  Id                          Date    Query          User  \\\n",
      "0         4   5  Mon May 11 03:18:54 UTC 2009  kindle2        chadfu   \n",
      "1         4   6  Mon May 11 03:19:04 UTC 2009  kindle2         SIX15   \n",
      "2         4   7  Mon May 11 03:21:41 UTC 2009  kindle2      yamarama   \n",
      "3         4   8  Mon May 11 03:22:00 UTC 2009  kindle2  GeorgeVHulme   \n",
      "4         0   9  Mon May 11 03:22:30 UTC 2009      aig       Seth937   \n",
      "\n",
      "                                                Text  \n",
      "0  Ok, first assesment of the #kindle2 ...it fuck...  \n",
      "1  @kenburbary You'll love your Kindle2. I've had...  \n",
      "2  @mikefish  Fair enough. But i have the Kindle2...  \n",
      "3  @richardebaker no. it is too big. I'm quite ha...  \n",
      "4  Fuck this economy. I hate aig and their non lo...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Datos originales\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renomrar las columnas para facilitar su uso\n",
    "df=df.rename(columns={'0':'polarity', '1':'id', '2':'date', '3':'query', '4':'user', '5':'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se limpia el texto\n",
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar menciones\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la limpieza al texto\n",
    "df['cleaned_text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Eliminar stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento completado. Los datos preprocesados se guardaron en: C:\\Users\\manue\\OneDrive\\Escritorio\\SentimentAnalysis\\output\\preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# 4. Guardar el dataset preprocesado\n",
    "output_file_path = r'C:\\Users\\manue\\OneDrive\\Escritorio\\SentimentAnalysis\\output\\preprocessed_data.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Preprocesamiento completado. Los datos preprocesados se guardaron en:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset preprocesado\n",
    "file_path = r'C:\\Users\\manue\\OneDrive\\Escritorio\\SentimentAnalysis\\output\\preprocessed_data.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización\n",
    "df['tokens'] = df['cleaned_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener el análisis de sentimientos\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    # Retornar la polaridad\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el análisis de sentimientos\n",
    "df['sentiment'] = df['cleaned_text'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(tokens) for tokens in df['tokens']])\n",
    "vocab = set([word for tokens in df['tokens'] for word in tokens])\n",
    "word_index = {word: i + 1 for i, word in enumerate(vocab)}  # +1 para reservar el índice 0\n",
    "X = [[word_index[word] for word in tokens] for tokens in df['tokens']]\n",
    "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "y = (df['sentiment'] > 0).astype(int)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5. Construir la red neuronal\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5955 - loss: 0.6838 - val_accuracy: 0.5300 - val_loss: 0.6804\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5943 - loss: 0.6642 - val_accuracy: 0.5300 - val_loss: 0.6774\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5884 - loss: 0.6566 - val_accuracy: 0.5400 - val_loss: 0.6741\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5710 - loss: 0.6573 - val_accuracy: 0.5500 - val_loss: 0.6699\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5780 - loss: 0.6416 - val_accuracy: 0.5600 - val_loss: 0.6645\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6158 - loss: 0.6277 - val_accuracy: 0.5900 - val_loss: 0.6581\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6847 - loss: 0.6034 - val_accuracy: 0.6000 - val_loss: 0.6526\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7168 - loss: 0.5814 - val_accuracy: 0.6000 - val_loss: 0.6450\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7511 - loss: 0.5632 - val_accuracy: 0.6200 - val_loss: 0.6358\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7817 - loss: 0.5536 - val_accuracy: 0.6400 - val_loss: 0.6273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x220e085ba90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6685 - loss: 0.6191 \n",
      "Accuracy: 0.6399999856948853\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Predicción: Negativo\n"
     ]
    }
   ],
   "source": [
    "# 8. Uso del modelo para nuevos textos\n",
    "new_text = \"Este producto es increíble, lo recomiendo ampliamente.\"\n",
    "new_tokens = word_tokenize(clean_text(new_text))\n",
    "new_sequence = pad_sequences([[word_index.get(word, 0) for word in new_tokens]], maxlen=max_length)\n",
    "sentiment = model.predict(new_sequence)[0][0]\n",
    "\n",
    "if sentiment > 0.5:\n",
    "    print(\"Predicción: Positivo\")\n",
    "else:\n",
    "    print(\"Predicción: Negativo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTHON_KERNEL",
   "language": "python",
   "name": "python_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
